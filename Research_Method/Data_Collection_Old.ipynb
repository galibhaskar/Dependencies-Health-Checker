{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Personal Access Token upload\n",
    "from ipywidgets import FileUpload, interact\n",
    "@interact(files=FileUpload())\n",
    "def set_token(files={}):\n",
    "    global token\n",
    "    if files:\n",
    "        for key, values in files.items():\n",
    "            token = values['content'].decode(\"utf-8\").strip()\n",
    "            print(\"Token Loaded!\")\n",
    "baseUrl=\"https://api.github.com/repos/TheAlgorithms/Python\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### token headers inclusion\n",
    "def token_auth(request):\n",
    "    request.headers[\"User-Agent\"] = \"RMS_Research_Project\"\n",
    "    request.headers[\"Authorization\"] = \"token {}\".format(token)\n",
    "    # print(f\"{request.headers}\")\n",
    "    return request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### generic get request service\n",
    "def get(url, requestType):\n",
    "    if requestType == \"GIT\":\n",
    "        response = requests.get(url,auth = token_auth)\n",
    "    else:\n",
    "        response = requests.get(url)\n",
    "    if(response.status_code==200):\n",
    "        data = response.json()\n",
    "        return data\n",
    "    print(\"Request:{} failed\".format(url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def isVersionActive(packageName):\n",
    "    packageInfo = get(f\"https://registry.npmjs.org/{packageName}\",\"Other\")\n",
    "    if packageInfo:\n",
    "        if 'deprecated' in packageInfo:\n",
    "            # print(f\"package:{packageName} is deprecated\")\n",
    "            # print(f\"deprecation message:{packageInfo['deprecated']}\")\n",
    "            # print(\"------------------------------------------------------------------------------------\")\n",
    "            return False\n",
    "        # print(\"package:{} is Active\".format(packageInfo['version'], packageName))\n",
    "        # print(\"------------------------------------------------------------------------------------\")\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decodeFileContent(content, encodingType):\n",
    "  file_content = \"\"\n",
    "  if encodingType == 'base64':\n",
    "    file_content = base64.b64decode(content).decode()\n",
    "  return file_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getLatestPackageInfo(packageName):\n",
    "  response = get(f\"https://registry.npmjs.org/{packageName}/latest\",\"Other\")\n",
    "  if \"dependencies\" not in response:\n",
    "      response[\"dependencies\"] = []\n",
    "  return {\"version\":response['version'],\"dependencies\":response[\"dependencies\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def getPayload(item, cols):\n",
    "  payload = {}\n",
    "  for col in cols:\n",
    "    payload[col]=item[col]\n",
    "  # print(payload)\n",
    "  return payload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fetching repos in between 2010 and Present range\n",
    "def fetchDatabyPageNumber(pageNumber):\n",
    "  data = get(f\"https://api.github.com/search/repositories?q=language:javascript&created%3A%3E2010-01-01&per_page=100&page={pageNumber}\", \"GIT\")\n",
    "  # data = get(f\"https://api.github.com/search/repositories?q=language:javascript&created%3A%3E2010-01-01&created%3A%3C2020-01-01&per_page=100&page={pageNumber}\")\n",
    "  # totalCount = data['total_count']\n",
    "  filteredData = []\n",
    "  if data and 'incomplete_results' in data:\n",
    "    print(data['incomplete_results'])\n",
    "    items = data['items']\n",
    "    filteredData = list(map(lambda item: getPayload(item, cols_to_extract), items))\n",
    "  return filteredData\n",
    "\n",
    "maxPageLimit = 10\n",
    "analysis_cols = ['up_to_date_count','upgradation_count','outdated_count','total_count','message']\n",
    "cols_to_extract = ['id','name','full_name','html_url','url','size','language','forks',\n",
    "                   'open_issues','visibility','watchers', 'created_at', 'updated_at', 'pushed_at']\n",
    "combined_cols = cols_to_extract + analysis_cols\n",
    "data = []\n",
    "for pageNumber in range(1, maxPageLimit+1):\n",
    "  data.extend(fetchDatabyPageNumber(pageNumber))\n",
    "print(data)\n",
    "dataFrame = pd.DataFrame(data,columns = combined_cols)\n",
    "dataFrame = dataFrame.drop_duplicates(subset=['id'], keep='last')\n",
    "dataFrame.head(5)\n",
    "# dataFrame.to_csv(\"dataset.csv\")\n",
    "\n",
    "print(len(dataFrame.index))\n",
    "filtered = dataFrame.loc[(dataFrame['forks'] >= dataFrame['forks'].mean()) | (dataFrame['watchers'] >= dataFrame['watchers'].mean())]\n",
    "print(f\"Matching criteria: {len(filtered.index)}\")\n",
    "\n",
    "repo_urls = filtered['url'].head(100)\n",
    "print(f\"Length of primary results scope: {len(repo_urls)}\")\n",
    "# filtered['url'].head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "total_count = 0\n",
    "upgradation_count = 0\n",
    "outdated_count= 0\n",
    "up_to_date_count = 0\n",
    "\n",
    "results_df = pd.DataFrame(filtered).head(100)\n",
    "print(len(dataFrame.index))\n",
    "print(len(results_df.index))\n",
    "# results_df = pd.DataFrame(columns = ['url','up_to_date_count','upgradation_count','outdated_count','total_count','comments'])\n",
    "\n",
    "MAX_DEPTH_COUNT = 2\n",
    "\n",
    "def resolve_package_version(package_version):\n",
    "  multi_versions = package_version.strip().split(\" \")\n",
    "\n",
    "  if len(multi_versions) != 1:\n",
    "    print(f\"Multi version package:{package_version}\")\n",
    "    package_version = multi_versions[1].strip()\n",
    "    print(f\"Revised version:{package_version}\")\n",
    "\n",
    "  multi_versions = package_version.strip().split(\" \")\n",
    "\n",
    "  if len(multi_versions) != 1:\n",
    "    print(f\"Multi version package:{package_version}\")\n",
    "    package_version = multi_versions[1].strip()\n",
    "    print(f\"Revised version:{package_version}\")\n",
    "\n",
    "  for symbol in ['^', '~', '>=', '<=', '<', '>']:\n",
    "    package_version = f\"{package_version.replace(symbol,'')}\"\n",
    "  \n",
    "  version_list = package_version.split(\".\")\n",
    "  \n",
    "  for index in range(len(version_list)):\n",
    "    if version_list[index].lower() == 'x':\n",
    "      version_list[index] = '0'\n",
    "  \n",
    "  while(len(version_list) < 3):\n",
    "    version_list.append('0')\n",
    "\n",
    "  resolved_version = \".\".join(version_list)\n",
    "\n",
    "  # print(f\"Resolved version:{resolved_version}\")\n",
    "\n",
    "  return resolved_version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyzeDependencies(parent_package, dependencies, depth):\n",
    "  global_instances = globals()\n",
    "  print(\"------------------------function definition-----------------------------------------------------------------------\")\n",
    "\n",
    "  print(f\"Dependencies Count: {len(dependencies)} of package:{parent_package}\")\n",
    "  _current_depth = depth\n",
    "  for dependencyName in dependencies:\n",
    "    global_instances['total_count'] +=1\n",
    "    is_version_active = True\n",
    "    latest_version = None\n",
    "    internal_dependencies_list = []\n",
    "\n",
    "    # print(\"---------------------------------------------------------------------------------------------------------------\")\n",
    "    # print(\"dependencyName:\",dependencyName)\n",
    "    # print(\"---------------------------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    current_version = dependencies[dependencyName]\n",
    "\n",
    "    skip_symbols = ['*','=']\n",
    "\n",
    "    is_package_version_skipped = any([current_version.startswith(symbol) for symbol in skip_symbols])\n",
    "\n",
    "    if not is_package_version_skipped:\n",
    "      res = getLatestPackageInfo(dependencyName)\n",
    "      \n",
    "      latest_version = res[\"version\"]\n",
    "      \n",
    "      internal_dependencies_list = res[\"dependencies\"]\n",
    "\n",
    "      # print(f\"Current version:{current_version} latest version:{latest_version}\")\n",
    "\n",
    "      resolved_package_version = resolve_package_version(current_version)\n",
    "      \n",
    "      package_name_version = f\"{dependencyName}/{resolved_package_version}\"\n",
    "\n",
    "      is_version_active = isVersionActive(package_name_version)\n",
    "      \n",
    "      # print(f\"package_name_version:{package_name_version} is {'Active' if is_version_active else 'Deprecated'}\")\n",
    "    \n",
    "    else:\n",
    "      print(f\"Package version {current_version} requirement met\")\n",
    "\n",
    "    if not is_version_active:\n",
    "      global_instances['outdated_count'] += 1\n",
    "      print(f\"Resolved version:{resolved_package_version}\")\n",
    "      print(f\"Deprecated:- Current version:{current_version} Package :{package_name_version} is Deprecated\")\n",
    "\n",
    "    elif latest_version and latest_version != resolved_package_version:\n",
    "      global_instances['upgradation_count'] += 1\n",
    "      print(f\"Resolved version:{resolved_package_version}\")\n",
    "      print(f\"Upgrade:- Current version:{current_version} latest version:{latest_version}\")\n",
    "\n",
    "    else:\n",
    "      global_instances['up_to_date_count'] += 1\n",
    "\n",
    "    # print(f\"Internal Dependencies Count: {len(internal_dependencies_list)}\")\n",
    "\n",
    "    if _current_depth > 1:\n",
    "      _current_depth -= 1\n",
    "      # print(f\"Internal dependencies of depth:{_current_depth}--------------------------------------------------------------\")\n",
    "      analyzeDependencies(dependencyName ,internal_dependencies_list, _current_depth)\n",
    "      # print(\"--------------END OF THE DEPENDENCY CHECK---------------------------------------------------------------------\")\n",
    "        \n",
    "    else:\n",
    "      # print(\"max depth reached\")\n",
    "      _current_depth = depth\n",
    "          \n",
    "  if depth == MAX_DEPTH_COUNT:\n",
    "    print(\"TOTAL COUNT:\",global_instances['total_count'])\n",
    "    print(\"UP_TO_DATE COUNT:\",global_instances['up_to_date_count'])\n",
    "    print(\"UPGRADATION COUNT:\", global_instances['upgradation_count'])\n",
    "    print(\"Outdated COUNT:\", global_instances['outdated_count'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_global_counts():\n",
    "  global_instances = globals()\n",
    "  global_instances['total_count'] = 0\n",
    "  global_instances['up_to_date_count'] = 0\n",
    "  global_instances['upgradation_count'] = 0\n",
    "  global_instances['outdated_count'] = 0\n",
    "  global_instances['message'] = \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(url):\n",
    "  try:\n",
    "    global_instances = globals()\n",
    "\n",
    "    data = get(f\"{url}/contents/package.json\", \"GIT\")\n",
    "\n",
    "    if not data or 'content' not in data:\n",
    "      _error_message = f\"No Package.json is found in the repository\"\n",
    "      print(_error_message)\n",
    "      global_instances['message'] = _error_message\n",
    "      # add_data_frame_entry(url,_error_message)\n",
    "      return\n",
    "    \n",
    "    file_content = data['content']\n",
    "    \n",
    "    file_content_encoding = data.get('encoding')\n",
    "    \n",
    "    decoded_package_json = json.loads(decodeFileContent(file_content,file_content_encoding))\n",
    "    \n",
    "    if 'dependencies' not in decoded_package_json:\n",
    "      _error_message = f\"No dependencies found in package.json\"\n",
    "      print(_error_message)\n",
    "      global_instances['message'] = _error_message\n",
    "      # add_data_frame_entry(url,_error_message)\n",
    "      return\n",
    "\n",
    "    dependencies = decoded_package_json['dependencies']\n",
    "    analyzeDependencies(\"ROOT\",dependencies, MAX_DEPTH_COUNT)\n",
    "    global_instances['message'] = ''\n",
    "    # add_data_frame_entry(url)\n",
    "  \n",
    "  except Exception as e:\n",
    "    _error_message = f\"Exception occured:{e}\"\n",
    "    print(_error_message)\n",
    "    global_instances['message'] = _error_message\n",
    "    # add_data_frame_entry(url,_error_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_global_counts():\n",
    "  global_instances = globals()\n",
    "  global_instances['total_count'] = 0\n",
    "  global_instances['up_to_date_count'] = 0\n",
    "  global_instances['upgradation_count'] = 0\n",
    "  global_instances['outdated_count'] = 0\n",
    "  global_instances['message'] = \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(url):\n",
    "  try:\n",
    "    global_instances = globals()\n",
    "\n",
    "    data = get(f\"{url}/contents/package.json\", \"GIT\")\n",
    "\n",
    "    if not data or 'content' not in data:\n",
    "      _error_message = f\"No Package.json is found in the repository\"\n",
    "      print(_error_message)\n",
    "      global_instances['message'] = _error_message\n",
    "      # add_data_frame_entry(url,_error_message)\n",
    "      return\n",
    "    \n",
    "    file_content = data['content']\n",
    "    \n",
    "    file_content_encoding = data.get('encoding')\n",
    "    \n",
    "    decoded_package_json = json.loads(decodeFileContent(file_content,file_content_encoding))\n",
    "    \n",
    "    if 'dependencies' not in decoded_package_json:\n",
    "      _error_message = f\"No dependencies found in package.json\"\n",
    "      print(_error_message)\n",
    "      global_instances['message'] = _error_message\n",
    "      # add_data_frame_entry(url,_error_message)\n",
    "      return\n",
    "\n",
    "    dependencies = decoded_package_json['dependencies']\n",
    "    analyzeDependencies(\"ROOT\",dependencies, MAX_DEPTH_COUNT)\n",
    "    global_instances['message'] = ''\n",
    "    # add_data_frame_entry(url)\n",
    "  \n",
    "  except Exception as e:\n",
    "    _error_message = f\"Exception occured:{e}\"\n",
    "    print(_error_message)\n",
    "    global_instances['message'] = _error_message\n",
    "    # add_data_frame_entry(url,_error_message)\n",
    "global_instances = globals()\n",
    "results_df_instance = global_instances['results_df']\n",
    "print(\"Results:\")\n",
    "results_df_instance.to_csv(\"Preliminary_Results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_global_counts():\n",
    "  global_instances = globals()\n",
    "  global_instances['total_count'] = 0\n",
    "  global_instances['up_to_date_count'] = 0\n",
    "  global_instances['upgradation_count'] = 0\n",
    "  global_instances['outdated_count'] = 0\n",
    "  global_instances['message'] = \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(url):\n",
    "  try:\n",
    "    global_instances = globals()\n",
    "\n",
    "    data = get(f\"{url}/contents/package.json\", \"GIT\")\n",
    "\n",
    "    if not data or 'content' not in data:\n",
    "      _error_message = f\"No Package.json is found in the repository\"\n",
    "      print(_error_message)\n",
    "      global_instances['message'] = _error_message\n",
    "      # add_data_frame_entry(url,_error_message)\n",
    "      return\n",
    "    \n",
    "    file_content = data['content']\n",
    "    \n",
    "    file_content_encoding = data.get('encoding')\n",
    "    \n",
    "    decoded_package_json = json.loads(decodeFileContent(file_content,file_content_encoding))\n",
    "    \n",
    "    if 'dependencies' not in decoded_package_json:\n",
    "      _error_message = f\"No dependencies found in package.json\"\n",
    "      print(_error_message)\n",
    "      global_instances['message'] = _error_message\n",
    "      # add_data_frame_entry(url,_error_message)\n",
    "      return\n",
    "\n",
    "    dependencies = decoded_package_json['dependencies']\n",
    "    analyzeDependencies(\"ROOT\",dependencies, MAX_DEPTH_COUNT)\n",
    "    global_instances['message'] = ''\n",
    "    # add_data_frame_entry(url)\n",
    "  \n",
    "  except Exception as e:\n",
    "    _error_message = f\"Exception occured:{e}\"\n",
    "    print(_error_message)\n",
    "    global_instances['message'] = _error_message\n",
    "    # add_data_frame_entry(url,_error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### token headers inclusion\n",
    "def token_auth(request):\n",
    "    request.headers[\"User-Agent\"] = \"RMS_Research_Project\"\n",
    "    request.headers[\"Authorization\"] = \"token {}\".format(token)\n",
    "    # print(f\"{request.headers}\")\n",
    "    return request\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### generic get request service\n",
    "def get(url, requestType):\n",
    "    if requestType == \"GIT\":\n",
    "        response = requests.get(url,auth = token_auth)\n",
    "    else:\n",
    "        response = requests.get(url)\n",
    "    if(response.status_code==200):\n",
    "        data = response.json()\n",
    "        return data\n",
    "    print(\"Request:{} failed\".format(url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def isVersionActive(packageName):\n",
    "    packageInfo = get(f\"https://registry.npmjs.org/{packageName}\",\"Other\")\n",
    "    if packageInfo:\n",
    "        if 'deprecated' in packageInfo:\n",
    "            # print(f\"package:{packageName} is deprecated\")\n",
    "            # print(f\"deprecation message:{packageInfo['deprecated']}\")\n",
    "            # print(\"------------------------------------------------------------------------------------\")\n",
    "            return False\n",
    "        # print(\"package:{} is Active\".format(packageInfo['version'], packageName))\n",
    "        # print(\"------------------------------------------------------------------------------------\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def decodeFileContent(content, encodingType):\n",
    "  file_content = \"\"\n",
    "  if encodingType == 'base64':\n",
    "    file_content = base64.b64decode(content).decode()\n",
    "  return file_content\n",
    "\n",
    "def getLatestPackageInfo(packageName):\n",
    "  response = get(f\"https://registry.npmjs.org/{packageName}/latest\",\"Other\")\n",
    "  if \"dependencies\" not in response:\n",
    "      response[\"dependencies\"] = []\n",
    "  return {\"version\":response['version'],\"dependencies\":response[\"dependencies\"]}\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def getPayload(item, cols):\n",
    "  payload = {}\n",
    "  for col in cols:\n",
    "    payload[col]=item[col]\n",
    "  # print(payload)\n",
    "  return payload\n",
    "\n",
    "# fetching repos in between 2010 and Present range\n",
    "def fetchDatabyPageNumber(pageNumber):\n",
    "  data = get(f\"https://api.github.com/search/repositories?q=language:javascript&created%3A%3E2010-01-01&per_page=100&page={pageNumber}\", \"GIT\")\n",
    "  # data = get(f\"https://api.github.com/search/repositories?q=language:javascript&created%3A%3E2010-01-01&created%3A%3C2020-01-01&per_page=100&page={pageNumber}\")\n",
    "  # totalCount = data['total_count']\n",
    "  filteredData = []\n",
    "  if data and 'incomplete_results' in data:\n",
    "    print(data['incomplete_results'])\n",
    "    items = data['items']\n",
    "    filteredData = list(map(lambda item: getPayload(item, cols_to_extract), items))\n",
    "  return filteredData\n",
    "\n",
    "maxPageLimit = 10\n",
    "analysis_cols = ['up_to_date_count','upgradation_count','outdated_count','total_count','message']\n",
    "cols_to_extract = ['id','name','full_name','html_url','url','size','language','forks',\n",
    "                   'open_issues','visibility','watchers', 'created_at', 'updated_at', 'pushed_at']\n",
    "combined_cols = cols_to_extract + analysis_cols\n",
    "data = []\n",
    "for pageNumber in range(1, maxPageLimit+1):\n",
    "  data.extend(fetchDatabyPageNumber(pageNumber))\n",
    "print(data)\n",
    "dataFrame = pd.DataFrame(data,columns = combined_cols)\n",
    "dataFrame = dataFrame.drop_duplicates(subset=['id'], keep='last')\n",
    "dataFrame.head(5)\n",
    "# dataFrame.to_csv(\"dataset.csv\")\n",
    "\n",
    "print(len(dataFrame.index))\n",
    "filtered = dataFrame.loc[(dataFrame['forks'] >= dataFrame['forks'].mean()) | (dataFrame['watchers'] >= dataFrame['watchers'].mean())]\n",
    "print(f\"Matching criteria: {len(filtered.index)}\")\n",
    "\n",
    "repo_urls = filtered['url'].head(100)\n",
    "print(f\"Length of primary results scope: {len(repo_urls)}\")\n",
    "# filtered['url'].head(5)\n",
    "import json\n",
    "\n",
    "total_count = 0\n",
    "upgradation_count = 0\n",
    "outdated_count= 0\n",
    "up_to_date_count = 0\n",
    "\n",
    "results_df = pd.DataFrame(filtered).head(100)\n",
    "print(len(dataFrame.index))\n",
    "print(len(results_df.index))\n",
    "# results_df = pd.DataFrame(columns = ['url','up_to_date_count','upgradation_count','outdated_count','total_count','comments'])\n",
    "\n",
    "MAX_DEPTH_COUNT = 2\n",
    "\n",
    "def resolve_package_version(package_version):\n",
    "  multi_versions = package_version.strip().split(\" \")\n",
    "\n",
    "  if len(multi_versions) != 1:\n",
    "    print(f\"Multi version package:{package_version}\")\n",
    "    package_version = multi_versions[1].strip()\n",
    "    print(f\"Revised version:{package_version}\")\n",
    "\n",
    "  multi_versions = package_version.strip().split(\" \")\n",
    "\n",
    "  if len(multi_versions) != 1:\n",
    "    print(f\"Multi version package:{package_version}\")\n",
    "    package_version = multi_versions[1].strip()\n",
    "    print(f\"Revised version:{package_version}\")\n",
    "\n",
    "  for symbol in ['^', '~', '>=', '<=', '<', '>']:\n",
    "    package_version = f\"{package_version.replace(symbol,'')}\"\n",
    "  \n",
    "  version_list = package_version.split(\".\")\n",
    "  \n",
    "  for index in range(len(version_list)):\n",
    "    if version_list[index].lower() == 'x':\n",
    "      version_list[index] = '0'\n",
    "  \n",
    "  while(len(version_list) < 3):\n",
    "    version_list.append('0')\n",
    "\n",
    "  resolved_version = \".\".join(version_list)\n",
    "\n",
    "  # print(f\"Resolved version:{resolved_version}\")\n",
    "\n",
    "  return resolved_version\n",
    "\n",
    "def analyzeDependencies(parent_package, dependencies, depth):\n",
    "  global_instances = globals()\n",
    "  print(\"------------------------function definition-----------------------------------------------------------------------\")\n",
    "\n",
    "  print(f\"Dependencies Count: {len(dependencies)} of package:{parent_package}\")\n",
    "  _current_depth = depth\n",
    "  for dependencyName in dependencies:\n",
    "    global_instances['total_count'] +=1\n",
    "    is_version_active = True\n",
    "    latest_version = None\n",
    "    internal_dependencies_list = []\n",
    "\n",
    "    # print(\"---------------------------------------------------------------------------------------------------------------\")\n",
    "    # print(\"dependencyName:\",dependencyName)\n",
    "    # print(\"---------------------------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    current_version = dependencies[dependencyName]\n",
    "\n",
    "    skip_symbols = ['*','=']\n",
    "\n",
    "    is_package_version_skipped = any([current_version.startswith(symbol) for symbol in skip_symbols])\n",
    "\n",
    "    if not is_package_version_skipped:\n",
    "      res = getLatestPackageInfo(dependencyName)\n",
    "      \n",
    "      latest_version = res[\"version\"]\n",
    "      \n",
    "      internal_dependencies_list = res[\"dependencies\"]\n",
    "\n",
    "      # print(f\"Current version:{current_version} latest version:{latest_version}\")\n",
    "\n",
    "      resolved_package_version = resolve_package_version(current_version)\n",
    "      \n",
    "      package_name_version = f\"{dependencyName}/{resolved_package_version}\"\n",
    "\n",
    "      is_version_active = isVersionActive(package_name_version)\n",
    "      \n",
    "      # print(f\"package_name_version:{package_name_version} is {'Active' if is_version_active else 'Deprecated'}\")\n",
    "    \n",
    "    else:\n",
    "      print(f\"Package version {current_version} requirement met\")\n",
    "\n",
    "    if not is_version_active:\n",
    "      global_instances['outdated_count'] += 1\n",
    "      print(f\"Resolved version:{resolved_package_version}\")\n",
    "      print(f\"Deprecated:- Current version:{current_version} Package :{package_name_version} is Deprecated\")\n",
    "\n",
    "    elif latest_version and latest_version != resolved_package_version:\n",
    "      global_instances['upgradation_count'] += 1\n",
    "      print(f\"Resolved version:{resolved_package_version}\")\n",
    "      print(f\"Upgrade:- Current version:{current_version} latest version:{latest_version}\")\n",
    "\n",
    "    else:\n",
    "      global_instances['up_to_date_count'] += 1\n",
    "\n",
    "    # print(f\"Internal Dependencies Count: {len(internal_dependencies_list)}\")\n",
    "\n",
    "    if _current_depth > 1:\n",
    "      _current_depth -= 1\n",
    "      # print(f\"Internal dependencies of depth:{_current_depth}--------------------------------------------------------------\")\n",
    "      analyzeDependencies(dependencyName ,internal_dependencies_list, _current_depth)\n",
    "      # print(\"--------------END OF THE DEPENDENCY CHECK---------------------------------------------------------------------\")\n",
    "        \n",
    "    else:\n",
    "      # print(\"max depth reached\")\n",
    "      _current_depth = depth\n",
    "          \n",
    "  if depth == MAX_DEPTH_COUNT:\n",
    "    print(\"TOTAL COUNT:\",global_instances['total_count'])\n",
    "    print(\"UP_TO_DATE COUNT:\",global_instances['up_to_date_count'])\n",
    "    print(\"UPGRADATION COUNT:\", global_instances['upgradation_count'])\n",
    "    print(\"Outdated COUNT:\", global_instances['outdated_count'])\n",
    "\n",
    "def reset_global_counts():\n",
    "  global_instances = globals()\n",
    "  global_instances['total_count'] = 0\n",
    "  global_instances['up_to_date_count'] = 0\n",
    "  global_instances['upgradation_count'] = 0\n",
    "  global_instances['outdated_count'] = 0\n",
    "  global_instances['message'] = \"\"\n",
    "\n",
    "def analyze(url):\n",
    "  try:\n",
    "    global_instances = globals()\n",
    "\n",
    "    data = get(f\"{url}/contents/package.json\", \"GIT\")\n",
    "\n",
    "    if not data or 'content' not in data:\n",
    "      _error_message = f\"No Package.json is found in the repository\"\n",
    "      print(_error_message)\n",
    "      global_instances['message'] = _error_message\n",
    "      # add_data_frame_entry(url,_error_message)\n",
    "      return\n",
    "    \n",
    "    file_content = data['content']\n",
    "    \n",
    "    file_content_encoding = data.get('encoding')\n",
    "    \n",
    "    decoded_package_json = json.loads(decodeFileContent(file_content,file_content_encoding))\n",
    "    \n",
    "    if 'dependencies' not in decoded_package_json:\n",
    "      _error_message = f\"No dependencies found in package.json\"\n",
    "      print(_error_message)\n",
    "      global_instances['message'] = _error_message\n",
    "      # add_data_frame_entry(url,_error_message)\n",
    "      return\n",
    "\n",
    "    dependencies = decoded_package_json['dependencies']\n",
    "    analyzeDependencies(\"ROOT\",dependencies, MAX_DEPTH_COUNT)\n",
    "    global_instances['message'] = ''\n",
    "    # add_data_frame_entry(url)\n",
    "  \n",
    "  except Exception as e:\n",
    "    _error_message = f\"Exception occured:{e}\"\n",
    "    print(_error_message)\n",
    "    global_instances['message'] = _error_message\n",
    "    # add_data_frame_entry(url,_error_message)\n",
    "def reset_global_counts():\n",
    "  global_instances = globals()\n",
    "  global_instances['total_count'] = 0\n",
    "  global_instances['up_to_date_count'] = 0\n",
    "  global_instances['upgradation_count'] = 0\n",
    "  global_instances['outdated_count'] = 0\n",
    "  global_instances['message'] = \"\"\n",
    "\n",
    "def analyze(url):\n",
    "  try:\n",
    "    global_instances = globals()\n",
    "\n",
    "    data = get(f\"{url}/contents/package.json\", \"GIT\")\n",
    "\n",
    "    if not data or 'content' not in data:\n",
    "      _error_message = f\"No Package.json is found in the repository\"\n",
    "      print(_error_message)\n",
    "      global_instances['message'] = _error_message\n",
    "      # add_data_frame_entry(url,_error_message)\n",
    "      return\n",
    "    \n",
    "    file_content = data['content']\n",
    "    \n",
    "    file_content_encoding = data.get('encoding')\n",
    "    \n",
    "    decoded_package_json = json.loads(decodeFileContent(file_content,file_content_encoding))\n",
    "    \n",
    "    if 'dependencies' not in decoded_package_json:\n",
    "      _error_message = f\"No dependencies found in package.json\"\n",
    "      print(_error_message)\n",
    "      global_instances['message'] = _error_message\n",
    "      # add_data_frame_entry(url,_error_message)\n",
    "      return\n",
    "\n",
    "    dependencies = decoded_package_json['dependencies']\n",
    "    analyzeDependencies(\"ROOT\",dependencies, MAX_DEPTH_COUNT)\n",
    "    global_instances['message'] = ''\n",
    "    # add_data_frame_entry(url)\n",
    "  \n",
    "  except Exception as e:\n",
    "    _error_message = f\"Exception occured:{e}\"\n",
    "    print(_error_message)\n",
    "    global_instances['message'] = _error_message\n",
    "    # add_data_frame_entry(url,_error_message)\n",
    "global_instances = globals()\n",
    "results_df_instance = global_instances['results_df']\n",
    "print(\"Results:\")\n",
    "results_df_instance.to_csv(\"Preliminary_Results.csv\")\n",
    "def reset_global_counts():\n",
    "  global_instances = globals()\n",
    "  global_instances['total_count'] = 0\n",
    "  global_instances['up_to_date_count'] = 0\n",
    "  global_instances['upgradation_count'] = 0\n",
    "  global_instances['outdated_count'] = 0\n",
    "  global_instances['message'] = \"\"\n",
    "\n",
    "def analyze(url):\n",
    "  try:\n",
    "    global_instances = globals()\n",
    "\n",
    "    data = get(f\"{url}/contents/package.json\", \"GIT\")\n",
    "\n",
    "    if not data or 'content' not in data:\n",
    "      _error_message = f\"No Package.json is found in the repository\"\n",
    "      print(_error_message)\n",
    "      global_instances['message'] = _error_message\n",
    "      # add_data_frame_entry(url,_error_message)\n",
    "      return\n",
    "    \n",
    "    file_content = data['content']\n",
    "    \n",
    "    file_content_encoding = data.get('encoding')\n",
    "    \n",
    "    decoded_package_json = json.loads(decodeFileContent(file_content,file_content_encoding))\n",
    "    \n",
    "    if 'dependencies' not in decoded_package_json:\n",
    "      _error_message = f\"No dependencies found in package.json\"\n",
    "      print(_error_message)\n",
    "      global_instances['message'] = _error_message\n",
    "      # add_data_frame_entry(url,_error_message)\n",
    "      return\n",
    "\n",
    "    dependencies = decoded_package_json['dependencies']\n",
    "    analyzeDependencies(\"ROOT\",dependencies, MAX_DEPTH_COUNT)\n",
    "    global_instances['message'] = ''\n",
    "    # add_data_frame_entry(url)\n",
    "  \n",
    "  except Exception as e:\n",
    "    _error_message = f\"Exception occured:{e}\"\n",
    "    print(_error_message)\n",
    "    global_instances['message'] = _error_message\n",
    "    # add_data_frame_entry(url,_error_message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9bed6b60f3f2351f8d67113307b2642d651e94ce8f500de393544803fb9bf5b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
